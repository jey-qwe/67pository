"""
RAG INGESTION SCRIPT - The Archivist
Converts user_data.md into vectors and stores locally
Uses local HuggingFace embeddings (no API required)
Uses FAISS for vector storage (no C++ build tools needed)
"""

import sys
import io
import os
from pathlib import Path
import shutil
import pickle

# --- –ñ–ï–õ–ï–ó–ù–´–ô –ö–£–ü–û–õ (–ù–ê–ß–ê–õ–û) ---

# 1. –ó–∞—Ç—ã–∫–∞–µ–º —Ä–æ—Ç OpenAI (—á—Ç–æ–±—ã CrewAI –¥–∞–∂–µ –Ω–µ –¥—É–º–∞–ª —Ç—É–¥–∞ —Å—Ç—É—á–∞—Ç—å—Å—è)
os.environ["OPENAI_API_KEY"] = "NA"
os.environ["OPENAI_MODEL_NAME"] = "NA"

# 2. –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è –∫–æ–¥–∏—Ä–æ–≤–∫–∞ (–ª–µ—á–∏—Ç –æ—à–∏–±–∫—É "invalid UTF-8" –∏–∑ –ª–æ–≥–æ–≤)
os.environ["PYTHONIOENCODING"] = "utf-8"
os.environ["LANG"] = "C.UTF-8"

# 3. –ü–µ—Ä–µ—Ö–≤–∞—Ç –≤—ã–≤–æ–¥–∞ (—á—Ç–æ–±—ã Windows –∫–æ–Ω—Å–æ–ª—å –Ω–µ –∫—Ä–∞—à–∏–ª–∞—Å—å –æ—Ç —ç–º–æ–¥–∑–∏)
if sys.platform == "win32":
    sys.stdout.reconfigure(encoding='utf-8')
    sys.stderr.reconfigure(encoding='utf-8')

# --- –ñ–ï–õ–ï–ó–ù–´–ô –ö–£–ü–û–õ (–ö–û–ù–ï–¶) ---

from langchain_community.document_loaders import TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS

# Import UTF-8 sanitization utilities
from utils import sanitize_for_grpc

# ============================================================================
# PATH CONFIGURATION
# ============================================================================

# Get script directory (src/)
SCRIPT_DIR = Path(__file__).parent

# Data file: ../data/user_data.md
DATA_FILE = SCRIPT_DIR.parent / "data" / "user_data.md"

# FAISS DB: ../faiss_db
FAISS_DIR = SCRIPT_DIR.parent / "faiss_db"

# ============================================================================
# CONFIGURATION
# ============================================================================

CHUNK_SIZE = 500
CHUNK_OVERLAP = 50
EMBEDDING_MODEL = "sentence-transformers/all-MiniLM-L6-v2"

# ============================================================================
# MAIN INGESTION LOGIC
# ============================================================================

def ensure_clean_db():
    """Remove existing FAISS DB to ensure idempotency"""
    if FAISS_DIR.exists():
        print(f"üóëÔ∏è  Removing existing database at {FAISS_DIR}")
        shutil.rmtree(FAISS_DIR)
        print("‚úÖ Old database removed")
    else:
        print("üìÇ No existing database found")


def load_user_data():
    """Load user_data.md from data directory"""
    print(f"\nüìñ Loading data from: {DATA_FILE}")
    
    if not DATA_FILE.exists():
        raise FileNotFoundError(f"‚ùå Data file not found: {DATA_FILE}")
    
    loader = TextLoader(str(DATA_FILE), encoding='utf-8')
    documents = loader.load()
    
    print(f"‚úÖ Loaded {len(documents)} document(s)")
    print(f"   Total characters: {sum(len(doc.page_content) for doc in documents)}")
    
    return documents


def split_documents(documents):
    """Split documents into chunks"""
    print(f"\n‚úÇÔ∏è  Splitting documents...")
    print(f"   Chunk size: {CHUNK_SIZE} characters")
    print(f"   Overlap: {CHUNK_OVERLAP} characters")
    
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=CHUNK_SIZE,
        chunk_overlap=CHUNK_OVERLAP,
        length_function=len,
        separators=["\n\n", "\n", ". ", " ", ""]
    )
    
    chunks = text_splitter.split_documents(documents)
    
    print(f"‚úÖ Created {len(chunks)} chunks")
    
    # Sanitize all chunks to prevent UTF-8 encoding errors
    print(f"\nüßπ Sanitizing text for UTF-8 compliance...")
    for chunk in chunks:
        chunk.page_content = sanitize_for_grpc(chunk.page_content)
    print(f"‚úÖ All chunks sanitized")
    
    # Show sample chunk
    if chunks:
        print(f"\nüìÑ Sample chunk:")
        preview = chunks[0].page_content[:100].replace('\n', ' ')
        print(f"   '{preview}...'")
    
    return chunks


def create_embeddings():
    """Initialize HuggingFace embeddings (local, no API)"""
    print(f"\nüß† Initializing embeddings model...")
    print(f"   Model: {EMBEDDING_MODEL}")
    print(f"   ‚ö° Running locally (no API required)")
    print(f"   ‚è≥ First run will download the model...")
    
    embeddings = HuggingFaceEmbeddings(
        model_name=EMBEDDING_MODEL,
        model_kwargs={'device': 'cpu'},  # Use 'cuda' if you have GPU
        encode_kwargs={'normalize_embeddings': True}
    )
    
    print("‚úÖ Embeddings model ready")
    
    return embeddings


def store_in_faiss(chunks, embeddings):
    """Store chunks in FAISS vector database"""
    print(f"\nüíæ Creating vector database...")
    print(f"   Location: {FAISS_DIR}")
    print(f"   Vector store: FAISS (local, high-performance)")
    
    # Create FAISS directory
    FAISS_DIR.mkdir(parents=True, exist_ok=True)
    
    # Create FAISS vector store
    vectorstore = FAISS.from_documents(
        documents=chunks,
        embedding=embeddings
    )
    
    # Save to disk
    vectorstore.save_local(str(FAISS_DIR))
    
    print(f"‚úÖ Stored {len(chunks)} chunks in FAISS")
    
    return vectorstore


def verify_storage(vectorstore):
    """Verify that vectors were stored correctly"""
    print(f"\nüîç Verifying storage...")
    
    # Test similarity search
    test_queries = [
        "AI Engineer",
        "Video editing",
        "Kazakhstan"
    ]
    
    print(f"‚úÖ Database is functional\n")
    
    for query in test_queries:
        results = vectorstore.similarity_search(query, k=2)
        
        print(f"üìä Test query: '{query}'")
        if results:
            preview = results[0].page_content[:100].replace('\n', ' ')
            print(f"   Top match: '{preview}...'")
        print()
    
    return True


def main():
    """Main ingestion pipeline"""
    print("=" * 70)
    print("üöÄ THE ARCHIVIST - RAG Ingestion Pipeline")
    print("=" * 70)
    print(f"üìç Working directory: {Path.cwd()}")
    print(f"üìç Script location: {SCRIPT_DIR}")
    
    try:
        # Step 1: Clean existing database
        ensure_clean_db()
        
        # Step 2: Load user data
        documents = load_user_data()
        
        # Step 3: Split into chunks
        chunks = split_documents(documents)
        
        # Step 4: Create embeddings
        embeddings = create_embeddings()
        
        # Step 5: Store in FAISS
        vectorstore = store_in_faiss(chunks, embeddings)
        
        # Step 6: Verify
        verify_storage(vectorstore)
        
        print("=" * 70)
        print("‚úÖ INGESTION COMPLETE - UnderDog Knowledge Base Ready!")
        print("=" * 70)
        print(f"üìÇ Database location: {FAISS_DIR.resolve()}")
        print(f"üéØ Ready for retrieval queries!")
        print(f"\nüí° Next step: Create query_memory.py to search your knowledge")
        print(f"üî• Your personal AI is one step closer to the Ocean!")
        
    except Exception as e:
        print(f"\n‚ùå ERROR: {type(e).__name__}")
        print(f"   {str(e)}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
